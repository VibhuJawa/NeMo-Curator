{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal 3-Format Writer -- Zero Data Loss Verification\n",
    "\n",
    "Verifies Parquet, WebDataset, and Lance for 10 fully-matched samples.\n",
    "Each format validated using canonical library. Extra columns (nv_*, match_*, etc.)\n",
    "verified present in ALL formats including WebDataset JSON."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import lance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import webdataset as wds\n",
    "from IPython.display import display, Image as IPImage\n",
    "from PIL import Image\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "OUTPUT_DIR = Path(\".\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Parquet (PyArrow + PIL)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pq_files = list((OUTPUT_DIR / \"parquet\").glob(\"*.parquet\"))\n",
    "pq_meta = pq.read_metadata(pq_files[0])\n",
    "print(f\"Rows: {pq_meta.num_rows}, Cols: {pq_meta.num_columns}, Row groups: {pq_meta.num_row_groups}\")\n",
    "table_pq = pq.read_table(pq_files[0])\n",
    "df_pq = table_pq.to_pandas()\n",
    "print(f\"Samples: {df_pq['sample_id'].nunique()}, Modalities: {df_pq['modality'].value_counts().to_dict()}\")\n",
    "nv_cols = [c for c in df_pq.columns if c.startswith('nv_')]\n",
    "print(f\"nv_* columns: {len(nv_cols)}, match_status: {df_pq['match_status'].value_counts(dropna=False).to_dict()}\")\n",
    "\n",
    "img_pq = df_pq[df_pq['modality'] == 'image']\n",
    "print(f\"\\nImage rows: {len(img_pq)}, with binary: {img_pq['binary_content'].notna().sum()}\")\n",
    "for _, row in img_pq.iterrows():\n",
    "    img = Image.open(BytesIO(row['binary_content']))\n",
    "    img.verify()\n",
    "print(\"PASS: All Parquet images decodable via PIL.verify()\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# nv_* data for image rows\n",
    "display(img_pq[['sample_id','position','nv_image_name','nv_width','nv_height','nv_img_byte_offset','nv_img_byte_size','match_status']].head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for sid in df_pq['sample_id'].unique()[:3]:\n",
    "    sample = df_pq[df_pq['sample_id'] == sid].sort_values('position')\n",
    "    short_sid = sid.split(':')[-1] if ':' in sid else sid[-30:]\n",
    "    print(f\"\\n--- Sample ...:{short_sid} ---\")\n",
    "    for _, row in sample[sample['modality'] != 'metadata'].iterrows():\n",
    "        pos = row['position']\n",
    "        if row['modality'] == 'text':\n",
    "            print(f\"  [pos={pos}] TEXT: {str(row['text_content'])[:100]}\")\n",
    "        elif row['modality'] == 'image':\n",
    "            blob = row['binary_content']\n",
    "            img = Image.open(BytesIO(blob))\n",
    "            print(f\"  [pos={pos}] IMAGE: {len(blob):,}B, {img.size[0]}x{img.size[1]} {img.format}\")\n",
    "            display(IPImage(data=blob, width=300))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. WebDataset (`webdataset` library + PIL + extra fields)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tar_files = list((OUTPUT_DIR / \"webdataset\").glob(\"*.tar\"))\n",
    "tar_path = str(tar_files[0])\n",
    "\n",
    "raw_ds = wds.WebDataset(tar_path, shardshuffle=False)\n",
    "raw_samples = list(raw_ds)\n",
    "print(f\"wds.WebDataset loaded {len(raw_samples)} samples\")\n",
    "for i, s in enumerate(raw_samples[:3]):\n",
    "    key = s.get('__key__', '?')\n",
    "    exts = sorted(k for k in s if not k.startswith('__'))\n",
    "    print(f\"  [{i}] key=...{key[-60:]}  members={exts[:5]}{'...' if len(exts)>5 else ''}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# .decode(\"pil\") -- canonical WebDataset image decode verification\n",
    "decoded_ds = wds.WebDataset(tar_path, shardshuffle=False).decode(\"pil\")\n",
    "decoded_samples = list(decoded_ds)\n",
    "total_images = 0\n",
    "for s in decoded_samples:\n",
    "    for k, v in s.items():\n",
    "        if isinstance(v, Image.Image):\n",
    "            total_images += 1\n",
    "            arr = np.array(v)\n",
    "            assert arr.ndim >= 2, f\"Bad image shape: {arr.shape}\"\n",
    "print(f\"PIL-decoded images: {total_images}\")\n",
    "print(\"PASS: All WebDataset images decode to PIL and convert to numpy\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interleaving + WebDataset compliance verification\n",
    "# Per spec: images array contains extension keys (what wds returns as dict keys)\n",
    "# so sample[images[pos]] directly retrieves the decoded image\n",
    "print(\"=== Interleaving + WebDataset Compliance ===\")\n",
    "for s in raw_samples:\n",
    "    key = s.get('__key__', '?')\n",
    "    payload = json.loads(s['json'])\n",
    "    texts = payload['texts']\n",
    "    images = payload['images']\n",
    "    assert len(texts) == len(images), f\"texts/images length mismatch for {key}\"\n",
    "\n",
    "    # Verify image refs are valid webdataset dict keys (extension-based)\n",
    "    wds_keys = {k for k in s if not k.startswith('__')}\n",
    "    for pos, ref in enumerate(images):\n",
    "        if ref is not None:\n",
    "            assert ref in wds_keys, (\n",
    "                f\"image ref '{ref}' at pos {pos} is NOT a valid wds sample key. \"\n",
    "                f\"Available keys: {sorted(wds_keys)}\"\n",
    "            )\n",
    "            # Verify the referenced bytes are actually an image\n",
    "            assert isinstance(s[ref], bytes), f\"sample['{ref}'] is not bytes\"\n",
    "            assert len(s[ref]) > 0, f\"sample['{ref}'] is empty\"\n",
    "\n",
    "    n_t = sum(1 for t in texts if t is not None)\n",
    "    n_i = sum(1 for i in images if i is not None)\n",
    "    print(f\"  ...{key[-60:]}: {len(texts)} pos, {n_t} texts, {n_i} imgs -- OK\")\n",
    "print(f\"PASS: All {len(raw_samples)} samples: interleaving valid, image refs are valid wds dict keys\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CRITICAL: Verify _row_extra preserves ALL extra columns, keyed by modality\n",
    "# Structure: _row_extra = {\"text\": [...], \"image\": [...], \"metadata\": {...}}\n",
    "# text[i] / image[i] align 1:1 with texts[i] / images[i]\n",
    "print(\"=== Extra Fields Verification (zero data loss) ===\")\n",
    "extra_col_names = None\n",
    "for s in raw_samples:\n",
    "    key = s.get('__key__', '?')\n",
    "    payload = json.loads(s['json'])\n",
    "    short_key = key[-50:]\n",
    "\n",
    "    assert '_row_extra' in payload, f\"FAIL: _row_extra missing in {short_key}\"\n",
    "    row_extra = payload['_row_extra']\n",
    "    assert 'text' in row_extra, f\"FAIL: _row_extra.text missing in {short_key}\"\n",
    "    assert 'image' in row_extra, f\"FAIL: _row_extra.image missing in {short_key}\"\n",
    "    assert 'metadata' in row_extra, f\"FAIL: _row_extra.metadata missing in {short_key}\"\n",
    "\n",
    "    texts = payload['texts']\n",
    "    images = payload['images']\n",
    "    text_extra = row_extra['text']\n",
    "    image_extra = row_extra['image']\n",
    "    meta_extra = row_extra['metadata']\n",
    "\n",
    "    assert len(text_extra) == len(texts), (\n",
    "        f\"FAIL: text_extra length {len(text_extra)} != texts {len(texts)} in {short_key}\"\n",
    "    )\n",
    "    assert len(image_extra) == len(images), (\n",
    "        f\"FAIL: image_extra length {len(image_extra)} != images {len(images)} in {short_key}\"\n",
    "    )\n",
    "\n",
    "    if extra_col_names is None:\n",
    "        for entry in image_extra:\n",
    "            if entry is not None:\n",
    "                extra_col_names = set(entry.keys())\n",
    "                break\n",
    "\n",
    "    # Verify image extra entries have nv_* data where images exist\n",
    "    for pos, img_ref in enumerate(images):\n",
    "        if img_ref is not None and image_extra[pos] is not None:\n",
    "            entry = image_extra[pos]\n",
    "            assert 'nv_width' in entry, f\"FAIL: nv_width missing in image extra pos={pos}\"\n",
    "            assert 'nv_height' in entry, f\"FAIL: nv_height missing in image extra pos={pos}\"\n",
    "            assert 'match_status' in entry, f\"FAIL: match_status missing in image extra pos={pos}\"\n",
    "        if img_ref is None:\n",
    "            assert image_extra[pos] is None, f\"FAIL: image_extra present at pos={pos} but no image\"\n",
    "\n",
    "    # Verify text extra aligns with texts array\n",
    "    for pos, txt in enumerate(texts):\n",
    "        if txt is not None:\n",
    "            assert text_extra[pos] is not None, f\"FAIL: text_extra null at pos={pos} but text exists\"\n",
    "        else:\n",
    "            assert text_extra[pos] is None, f\"FAIL: text_extra present at pos={pos} but no text\"\n",
    "\n",
    "    n_txt = sum(1 for e in text_extra if e is not None)\n",
    "    n_img = sum(1 for e in image_extra if e is not None)\n",
    "    print(f\"  ...{short_key}: text_extra={n_txt}, image_extra={n_img}, meta_extra={len(meta_extra)} fields -- OK\")\n",
    "\n",
    "print(f\"\\nExtra column names: {sorted(extra_col_names) if extra_col_names else 'none'}\")\n",
    "print(f\"PASS: _row_extra.text/image/metadata present with nv_*/match_* in all {len(raw_samples)} samples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cross-check: use images array as wds dict key to get bytes, compare dimensions to _row_extra\n",
    "print(\"=== WDS compliance: sample[images[pos]] lookup + nv dimension cross-check ===\")\n",
    "mismatches = 0\n",
    "lookup_failures = 0\n",
    "for s in raw_samples:\n",
    "    payload = json.loads(s['json'])\n",
    "    image_extra = payload['_row_extra']['image']\n",
    "    images = payload['images']\n",
    "    for pos, img_ref in enumerate(images):\n",
    "        if img_ref is None:\n",
    "            continue\n",
    "        # This is the key test: images[pos] must be a valid wds sample dict key\n",
    "        raw_bytes = s.get(img_ref)\n",
    "        if raw_bytes is None:\n",
    "            lookup_failures += 1\n",
    "            print(f\"  LOOKUP FAIL: sample['{img_ref}'] returned None\")\n",
    "            continue\n",
    "        img = Image.open(BytesIO(raw_bytes))\n",
    "        if image_extra[pos] is not None:\n",
    "            wds_w = image_extra[pos].get('nv_width')\n",
    "            wds_h = image_extra[pos].get('nv_height')\n",
    "            if wds_w is not None and (img.size[0] != int(wds_w) or img.size[1] != int(wds_h)):\n",
    "                mismatches += 1\n",
    "                print(f\"  DIM MISMATCH: actual={img.size} nv=({wds_w},{wds_h})\")\n",
    "assert lookup_failures == 0, f\"{lookup_failures} image lookups failed!\"\n",
    "print(f\"Lookup failures: {lookup_failures}, Dimension mismatches: {mismatches}\")\n",
    "print(\"PASS: sample[images[pos]] works for all images (WebDataset spec compliant)\")\n",
    "if mismatches == 0:\n",
    "    print(\"PASS: nv_width/nv_height match actual decoded dimensions\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display interleaved WebDataset samples using standard sample[images[pos]] lookup\n",
    "for s in decoded_samples[:2]:\n",
    "    key = s.get('__key__', '?')\n",
    "    raw_json = s.get('json')\n",
    "    if isinstance(raw_json, bytes):\n",
    "        raw_json = raw_json.decode('utf-8')\n",
    "    payload = json.loads(raw_json) if isinstance(raw_json, str) else raw_json\n",
    "    texts = payload['texts']\n",
    "    images = payload['images']\n",
    "    print(f\"\\n--- ...{key[-50:]} ---\")\n",
    "    for pos in range(len(texts)):\n",
    "        t = texts[pos]\n",
    "        img_ref = images[pos]\n",
    "        if t:\n",
    "            print(f\"  [pos={pos}] TEXT: {t[:100]}\")\n",
    "        if img_ref:\n",
    "            pil_img = s.get(img_ref)\n",
    "            if isinstance(pil_img, Image.Image):\n",
    "                ext = img_ref.split('.')[-1]\n",
    "                print(f\"  [pos={pos}] IMAGE: {pil_img.size[0]}x{pil_img.size[1]} (key='{img_ref}')\")\n",
    "                buf = BytesIO()\n",
    "                fmt = 'JPEG' if ext in ('jpg','jpeg') else ext.upper()\n",
    "                pil_img.save(buf, format=fmt)\n",
    "                display(IPImage(data=buf.getvalue(), width=300))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Lance (`lance` library + PIL)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lance_dirs = list((OUTPUT_DIR / \"lance\").glob(\"*.lance\"))\n",
    "ds = lance.dataset(str(lance_dirs[0]))\n",
    "print(f\"Rows: {ds.count_rows()}, Fragments: {len(ds.get_fragments())}, Version: {ds.version}\")\n",
    "for mod in ['text', 'image', 'metadata']:\n",
    "    filt = f\"modality = '{mod}'\"\n",
    "    print(f\"  {mod}: {ds.count_rows(filter=filt)}\")\n",
    "sample_ids = ds.to_table(columns=['sample_id']).column('sample_id').to_pylist()\n",
    "print(f\"  Unique samples: {len(set(sample_ids))}\")\n",
    "print(f\"\\nSchema ({len(ds.schema)} fields):\")\n",
    "print(ds.schema)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filtered scan: image rows with nv_* columns\n",
    "img_scan = ds.to_table(\n",
    "    columns=['sample_id','position','content_type','binary_content',\n",
    "             'nv_image_name','nv_width','nv_height','nv_img_byte_offset','nv_img_byte_size'],\n",
    "    filter=\"modality = 'image'\"\n",
    ")\n",
    "df_lance_imgs = img_scan.to_pandas()\n",
    "print(f\"Image rows: {len(df_lance_imgs)}, all binary: {df_lance_imgs['binary_content'].notna().all()}\")\n",
    "display(df_lance_imgs[['sample_id','position','nv_image_name','nv_width','nv_height']].head(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PIL verify + dimension cross-check\n",
    "decode_errs = 0\n",
    "dim_mismatches = 0\n",
    "for _, row in df_lance_imgs.iterrows():\n",
    "    blob = row['binary_content']\n",
    "    try:\n",
    "        img = Image.open(BytesIO(blob))\n",
    "        img.verify()\n",
    "    except Exception as e:\n",
    "        decode_errs += 1\n",
    "        continue\n",
    "    img = Image.open(BytesIO(blob))\n",
    "    nv_w = int(row['nv_width']) if pd.notna(row['nv_width']) else None\n",
    "    nv_h = int(row['nv_height']) if pd.notna(row['nv_height']) else None\n",
    "    if nv_w is not None and (img.size[0] != nv_w or img.size[1] != nv_h):\n",
    "        dim_mismatches += 1\n",
    "print(f\"Decode errors: {decode_errs}, Dimension mismatches: {dim_mismatches}\")\n",
    "assert decode_errs == 0\n",
    "print(\"PASS: All Lance images valid, nv dimensions match\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_lance = ds.to_table().to_pandas()\n",
    "for sid in df_lance['sample_id'].unique()[:2]:\n",
    "    sample = df_lance[df_lance['sample_id'] == sid].sort_values('position')\n",
    "    short_sid = sid.split(':')[-1] if ':' in sid else sid[-30:]\n",
    "    print(f\"\\n--- Sample ...:{short_sid} ---\")\n",
    "    for _, row in sample[sample['modality'] != 'metadata'].iterrows():\n",
    "        pos = row['position']\n",
    "        if row['modality'] == 'text':\n",
    "            print(f\"  [pos={pos}] TEXT: {str(row['text_content'])[:100]}\")\n",
    "        elif row['modality'] == 'image':\n",
    "            blob = row['binary_content']\n",
    "            img = Image.open(BytesIO(blob))\n",
    "            print(f\"  [pos={pos}] IMAGE: {len(blob):,}B, {img.size[0]}x{img.size[1]}\")\n",
    "            display(IPImage(data=blob, width=300))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Cross-Format Zero Data Loss Check"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parquet vs Lance: identical shapes, columns, binary content\n",
    "assert df_pq.shape == df_lance.shape\n",
    "assert set(df_pq.columns) == set(df_lance.columns)\n",
    "for col in ['sample_id','position','modality','content_type','text_content',\n",
    "            'match_status','match_tier','nv_image_name','nv_width','nv_height']:\n",
    "    assert df_pq[col].fillna('__NULL__').tolist() == df_lance[col].fillna('__NULL__').tolist(), f\"{col} differs\"\n",
    "\n",
    "pq_imgs_s = df_pq[df_pq['modality']=='image'].sort_values(['sample_id','position']).reset_index(drop=True)\n",
    "lance_imgs_s = df_lance[df_lance['modality']=='image'].sort_values(['sample_id','position']).reset_index(drop=True)\n",
    "for i in range(len(pq_imgs_s)):\n",
    "    assert pq_imgs_s.loc[i,'binary_content'] == lance_imgs_s.loc[i,'binary_content'], f\"Binary mismatch row {i}\"\n",
    "\n",
    "# WebDataset: correct sample/image counts + extra fields present\n",
    "wds_img_count = sum(\n",
    "    sum(1 for im in json.loads(s['json']).get('images',[]) if im is not None)\n",
    "    for s in raw_samples\n",
    ")\n",
    "assert len(raw_samples) == df_pq['sample_id'].nunique(), \"WDS sample count mismatch\"\n",
    "assert wds_img_count == len(pq_imgs_s), \"WDS image count mismatch\"\n",
    "\n",
    "# Verify WDS extra columns match Parquet extra columns\n",
    "pq_extra_cols = set(c for c in df_pq.columns if c not in {\n",
    "    'sample_id','position','modality','content_type','text_content',\n",
    "    'binary_content','source_ref','metadata_json','materialize_error'\n",
    "})\n",
    "wds_extra_cols = extra_col_names if extra_col_names else set()\n",
    "missing_in_wds = pq_extra_cols - wds_extra_cols\n",
    "assert not missing_in_wds, f\"Columns in Parquet but missing from WDS _row_extra: {missing_in_wds}\"\n",
    "\n",
    "print(\"=== FINAL RESULTS ===\")\n",
    "print(f\"  Parquet:    {len(df_pq)} rows, {df_pq['sample_id'].nunique()} samples, {len(pq_imgs_s)} imgs, {len(df_pq.columns)} cols\")\n",
    "print(f\"  Lance:      {ds.count_rows()} rows, {len(set(sample_ids))} samples, {len(lance_imgs_s)} imgs, {len(ds.schema)} cols\")\n",
    "print(f\"  WebDataset: {len(raw_samples)} samples, {wds_img_count} imgs, {len(wds_extra_cols)} extra cols in JSON\")\n",
    "print(f\"  Extra columns: {len(pq_extra_cols)} in Parquet, {len(wds_extra_cols)} in WDS _row_extra\")\n",
    "print()\n",
    "print(\"PASS: Parquet <-> Lance byte-identical (all columns + binary)\")\n",
    "print(\"PASS: WebDataset sample/image counts match\")\n",
    "print(\"PASS: WebDataset _row_extra has ALL extra columns (nv_*, match_*, etc.)\")\n",
    "print(\"PASS: All images PIL-verified across all 3 formats\")\n",
    "print(\"PASS: nv_width/nv_height match decoded dimensions (Lance + WDS)\")\n",
    "print(\"PASS: Interleaving preserved in WebDataset\")\n",
    "print(\"PASS: ZERO DATA LOSS across all formats\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. User Experience: Filtering by Metadata Fields\n",
    "\n",
    "Shows how a downstream consumer can filter by image size, match tier, domain, etc.\n",
    "across all 3 formats -- using each format's native strengths."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- 5a. Lance: filtered scan + pandas post-filter ---\nMIN_WIDTH = 400\nMIN_HEIGHT = 400\n\n# Read image rows with column projection (only loads needed columns)\nimg_table = ds.to_table(\n    columns=['sample_id', 'position', 'binary_content', 'nv_width', 'nv_height', 'nv_image_name'],\n    filter=\"modality = 'image'\"\n)\ndf_all_imgs = img_table.to_pandas()\ndf_all_imgs['nv_width_int'] = pd.to_numeric(df_all_imgs['nv_width'], errors='coerce')\ndf_all_imgs['nv_height_int'] = pd.to_numeric(df_all_imgs['nv_height'], errors='coerce')\ndf_large = df_all_imgs[\n    (df_all_imgs['nv_width_int'] >= MIN_WIDTH) & (df_all_imgs['nv_height_int'] >= MIN_HEIGHT)\n].copy()\n\nprint(f\"Lance: images >= {MIN_WIDTH}x{MIN_HEIGHT}\")\nprint(f\"  Matched: {len(df_large)} of {len(df_all_imgs)} total images\")\ndisplay(df_large[['sample_id', 'position', 'nv_image_name', 'nv_width', 'nv_height']].reset_index(drop=True))\n\nprint(f\"\\nShowing filtered images:\")\nfor _, row in df_large.head(3).iterrows():\n    blob = row['binary_content']\n    img = Image.open(BytesIO(blob))\n    short_sid = row['sample_id'].split(':')[-1] if ':' in row['sample_id'] else row['sample_id'][-20:]\n    print(f\"  ...{short_sid} pos={row['position']}: {img.size[0]}x{img.size[1]}\")\n    display(IPImage(data=blob, width=300))\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- 5b. Lance: get complete samples (all modalities) for filtered images ---\nlarge_sids = set(df_large['sample_id'].unique())\ndf_full = ds.to_table().to_pandas()\ndf_filtered_samples = df_full[df_full['sample_id'].isin(large_sids)]\nprint(f\"Samples with >= 1 image >= {MIN_WIDTH}x{MIN_HEIGHT}: {len(large_sids)}\")\nprint(f\"Total rows for those samples: {len(df_filtered_samples)}\")\nfor mod in ['text', 'image', 'metadata']:\n    print(f\"  {mod}: {(df_filtered_samples['modality']==mod).sum()}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- 5c. Parquet: column pushdown + pandas filter ---\n# Read only the columns we need, then filter in pandas\ncols_needed = ['sample_id', 'position', 'modality', 'nv_width', 'nv_height', 'nv_image_name']\ndf_pq_slim = pd.read_parquet(pq_files[0], columns=cols_needed)\ndf_pq_imgs = df_pq_slim[df_pq_slim['modality'] == 'image'].copy()\ndf_pq_imgs['w'] = pd.to_numeric(df_pq_imgs['nv_width'], errors='coerce')\ndf_pq_imgs['h'] = pd.to_numeric(df_pq_imgs['nv_height'], errors='coerce')\ndf_pq_large = df_pq_imgs[(df_pq_imgs['w'] >= MIN_WIDTH) & (df_pq_imgs['h'] >= MIN_HEIGHT)]\n\nprint(f\"Parquet column projection: images >= {MIN_WIDTH}x{MIN_HEIGHT}\")\nprint(f\"  Matched: {len(df_pq_large)} of {len(df_pq_imgs)} total images\")\ndisplay(df_pq_large[['sample_id', 'position', 'nv_image_name', 'nv_width', 'nv_height']].head(5).reset_index(drop=True))\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- 5d. WebDataset: streaming filter via _row_extra metadata ---\nMIN_W, MIN_H = 400, 400\n\ndef has_large_image(sample):\n    \"\"\"Pre-decode filter: json is still raw bytes here.\"\"\"\n    raw_json = sample.get('json')\n    if isinstance(raw_json, bytes):\n        raw_json = raw_json.decode('utf-8')\n    payload = json.loads(raw_json)\n    image_extra = payload.get('_row_extra', {}).get('image', [])\n    for entry in image_extra:\n        if entry is None:\n            continue\n        w = entry.get('nv_width')\n        h = entry.get('nv_height')\n        if w is not None and h is not None and float(w) >= MIN_W and float(h) >= MIN_H:\n            return True\n    return False\n\ndef get_payload(sample):\n    \"\"\"Handle json field being str, bytes, or already-decoded dict.\"\"\"\n    raw = sample.get('json')\n    if isinstance(raw, dict):\n        return raw\n    if isinstance(raw, bytes):\n        raw = raw.decode('utf-8')\n    return json.loads(raw)\n\nfiltered_ds = wds.WebDataset(tar_path, shardshuffle=False).select(has_large_image).decode(\"pil\")\nfiltered_samples = list(filtered_ds)\nprint(f\"WebDataset streaming filter: samples with image >= {MIN_W}x{MIN_H}\")\nprint(f\"  Before: {len(decoded_samples)} samples, After: {len(filtered_samples)} samples\")\n\nfor s in filtered_samples[:2]:\n    key = s['__key__']\n    payload = get_payload(s)\n    image_extra = payload['_row_extra']['image']\n    images = payload['images']\n    print(f\"\\n  ...{key[-50:]}:\")\n    for pos, ref in enumerate(images):\n        if ref is None:\n            continue\n        pil_img = s.get(ref)\n        extra = image_extra[pos] if image_extra[pos] else {}\n        w = extra.get('nv_width', '?')\n        h = extra.get('nv_height', '?')\n        if isinstance(pil_img, Image.Image):\n            is_large = isinstance(w, (int,float)) and w >= MIN_W and isinstance(h, (int,float)) and h >= MIN_H\n            marker = \" << LARGE\" if is_large else \"\"\n            print(f\"    pos={pos}: {pil_img.size[0]}x{pil_img.size[1]} (nv: {w}x{h}){marker}\")\n            buf = BytesIO()\n            ext_name = ref.split('.')[-1]\n            pil_img.save(buf, format='JPEG' if ext_name in ('jpg','jpeg') else ext_name.upper())\n            display(IPImage(data=buf.getvalue(), width=250))",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}