# Copyright (c) 2026, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import json
import mimetypes
import tarfile
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import fsspec
import pyarrow as pa

from nemo_curator.core.utils import split_table_by_group_max_bytes
from nemo_curator.stages.multimodal.utils import (
    DEFAULT_IMAGE_EXTENSIONS,
    DEFAULT_JSON_EXTENSIONS,
    require_source_id_field,
    resolve_storage_options,
    validate_and_project_source_fields,
)
from nemo_curator.tasks import FileGroupTask, MultiBatchTask
from nemo_curator.tasks.multimodal import MULTIMODAL_SCHEMA, RESERVED_COLUMNS

from .base import BaseMultimodalReader


@dataclass
class _ReadContext:
    """Per-tar state shared across all members in a single tar archive."""

    tar_path: str
    member_names: set[str]
    member_info: dict[str, tarfile.TarInfo]
    storage_options: dict[str, object]
    byte_cache: dict[str, bytes | None]


@dataclass
class _SampleContext:
    """Per-sample state passed to row builder methods."""

    sample_id: str
    sample: dict[str, Any]
    tar_path: str
    json_member_name: str
    member_names: set[str]
    member_info: dict[str, tarfile.TarInfo] | None
    passthrough: dict[str, Any]


@dataclass
class WebdatasetReaderStage(BaseMultimodalReader):
    """Read MINT1T-style WebDataset shards into a row-wise multimodal task."""

    materialize_on_read: bool = False
    max_batch_bytes: int | None = None
    json_extensions: tuple[str, ...] = DEFAULT_JSON_EXTENSIONS
    image_extensions: tuple[str, ...] = field(default_factory=lambda: DEFAULT_IMAGE_EXTENSIONS)
    source_id_field: str = ""
    sample_id_field: str | None = None
    texts_field: str = "texts"
    images_field: str = "images"
    image_member_field: str | None = None
    fields: tuple[str, ...] | None = None
    name: str = "webdataset_reader"

    def __post_init__(self) -> None:
        self.source_id_field = require_source_id_field(self.source_id_field)

    # -- source_ref construction --

    def _build_source_ref(
        self, ctx: _SampleContext, content_key: str | None, *, frame_index: int | None = None,
    ) -> str:
        if content_key is None:
            return MultiBatchTask.build_source_ref(path=None, member=None)
        byte_offset = None
        byte_size = None
        if ctx.member_info and content_key in ctx.member_info:
            info = ctx.member_info[content_key]
            byte_offset = info.offset_data
            byte_size = info.size
        return MultiBatchTask.build_source_ref(
            path=ctx.tar_path, member=content_key,
            byte_offset=byte_offset, byte_size=byte_size, frame_index=frame_index,
        )

    # -- row builders (override in subclasses for custom formats) --

    @staticmethod
    def _build_row(ctx: _SampleContext, row_fields: dict[str, Any]) -> dict[str, Any]:
        return {
            "sample_id": ctx.sample_id,
            "position": row_fields.get("position"),
            "modality": row_fields.get("modality"),
            "content_type": row_fields.get("content_type"),
            "text_content": row_fields.get("text_content"),
            "binary_content": row_fields.get("binary_content"),
            "source_ref": row_fields.get("source_ref"),
            "metadata_json": row_fields.get("metadata_json"),
            "materialize_error": None,
            **ctx.passthrough,
        }

    def _metadata_row(self, ctx: _SampleContext) -> dict[str, Any]:
        return self._build_row(ctx, {
            "position": -1,
            "modality": "metadata",
            "content_type": "application/json",
            "source_ref": self._build_source_ref(ctx, ctx.json_member_name),
            "metadata_json": json.dumps(
                {
                    **ctx.sample,
                    "_sample_source": {
                        "source_shard": Path(ctx.tar_path).name,
                        "tar_path": ctx.tar_path,
                        "json_member_name": ctx.json_member_name,
                    },
                },
                ensure_ascii=True,
            ),
        })

    def _text_rows(self, ctx: _SampleContext) -> list[dict[str, Any]]:
        texts = ctx.sample.get(self.texts_field)
        if not isinstance(texts, list):
            return []
        source_ref = self._build_source_ref(ctx, ctx.json_member_name)
        return [
            self._build_row(ctx, {
                "position": idx,
                "modality": "text",
                "content_type": "text/plain",
                "text_content": str(text_value),
                "source_ref": source_ref,
            })
            for idx, text_value in enumerate(texts)
            if text_value is not None
        ]

    def _image_rows(self, ctx: _SampleContext) -> list[dict[str, Any]]:
        images = ctx.sample.get(self.images_field)
        if not isinstance(images, list):
            return []
        image_member_name = self._resolve_default_image_member_name(
            ctx.sample_id, ctx.sample, images, ctx.member_names,
        )
        rows: list[dict[str, Any]] = []
        frame_counter = 0
        for idx, image_token in enumerate(images):
            if image_token is None:
                continue
            content_key = self._resolve_image_content_key(image_token, image_member_name, ctx.member_names)
            content_type, _ = mimetypes.guess_type(content_key or image_member_name or "")
            frame_index = None
            is_multiframe_candidate = content_type == "image/tiff"
            if content_key is not None and is_multiframe_candidate:
                frame_index = frame_counter
                frame_counter += 1
            rows.append(self._build_row(ctx, {
                "position": idx,
                "modality": "image",
                "content_type": content_type or ("application/octet-stream" if image_member_name else None),
                "source_ref": self._build_source_ref(ctx, content_key, frame_index=frame_index),
            }))
        return rows

    # -- sample-level orchestration --

    def _rows_from_sample(self, ctx: _SampleContext) -> list[dict[str, Any]]:
        rows: list[dict[str, Any]] = []
        rows.append(self._metadata_row(ctx))
        rows.extend(self._text_rows(ctx))
        rows.extend(self._image_rows(ctx))
        return rows

    # -- passthrough / schema helpers --

    def _build_passthrough_row(self, sample: dict[str, Any]) -> dict[str, Any]:
        excluded = RESERVED_COLUMNS | {
            self.source_id_field,
            *([self.sample_id_field] if self.sample_id_field else []),
            self.texts_field,
            self.images_field,
            *([self.image_member_field] if self.image_member_field else []),
        }
        return validate_and_project_source_fields(sample=sample, fields=self.fields, excluded_fields=excluded)

    def _empty_output_schema(self) -> pa.Schema:
        schema = MULTIMODAL_SCHEMA
        if not self.fields:
            return schema
        existing = set(schema.names)
        passthrough_fields = [pa.field(name, pa.null()) for name in self.fields if name not in existing]
        return pa.schema([*schema, *passthrough_fields]) if passthrough_fields else schema

    @staticmethod
    def _reconcile_schema(inferred: pa.Schema) -> pa.Schema:
        """Build a schema with canonical types for reserved columns and inferred types for passthrough."""
        canonical = {f.name: f for f in MULTIMODAL_SCHEMA}
        fields = []
        for f in inferred:
            if f.name in canonical:
                fields.append(canonical[f.name])
            else:
                fields.append(f)
        return pa.schema(fields)

    # -- image member resolution --

    def _resolve_default_image_member_name(
        self,
        sample_id: str,
        sample: dict[str, Any],
        images: list[object] | None,
        member_names: set[str],
    ) -> str | None:
        if self.image_member_field:
            image_member_name = sample.get(self.image_member_field)
            if isinstance(image_member_name, str) and image_member_name in member_names:
                return image_member_name
        if isinstance(images, list):
            for image_token in images:
                if isinstance(image_token, str) and image_token in member_names:
                    return image_token
        return next(
            (f"{sample_id}{ext}" for ext in self.image_extensions if f"{sample_id}{ext}" in member_names), None
        )

    @staticmethod
    def _resolve_image_content_key(
        image_token: object,
        default_image_member_name: str | None,
        member_names: set[str],
    ) -> str | None:
        if image_token is None:
            return None
        if isinstance(image_token, str) and image_token in member_names:
            return image_token
        return default_image_member_name

    # -- tar member extraction --

    @staticmethod
    def _extract_tar_member(tf: tarfile.TarFile, member_name: str, cache: dict[str, bytes | None]) -> bytes | None:
        if member_name in cache:
            return cache[member_name]
        try:
            extracted = tf.extractfile(member_name)
        except KeyError:
            extracted = None
        payload = extracted.read() if extracted is not None else None
        cache[member_name] = payload
        return payload

    # -- per-member processing --

    def _rows_from_member(
        self,
        tf: tarfile.TarFile,
        member: tarfile.TarInfo,
        read_ctx: _ReadContext,
    ) -> list[dict[str, Any]]:
        extracted = tf.extractfile(member)
        if extracted is None:
            return []
        payload = json.load(extracted)
        sample_id = (
            str(payload.get(self.sample_id_field))
            if self.sample_id_field and payload.get(self.sample_id_field) is not None
            else Path(member.name).stem
        )
        ctx = _SampleContext(
            sample_id=sample_id,
            sample=payload,
            tar_path=read_ctx.tar_path,
            json_member_name=member.name,
            member_names=read_ctx.member_names,
            member_info=read_ctx.member_info,
            passthrough=self._build_passthrough_row(payload),
        )
        sample_rows = self._rows_from_sample(ctx)
        if self.materialize_on_read:
            for row in sample_rows:
                if row["modality"] != "image" or row["position"] < 0:
                    continue
                parsed_ref = MultiBatchTask.parse_source_ref(row["source_ref"])
                content_key = parsed_ref.get("member")
                if content_key:
                    row["binary_content"] = self._extract_tar_member(tf, content_key, read_ctx.byte_cache)
            read_ctx.byte_cache.clear()
        return sample_rows

    # -- main entry point --

    def process(self, task: FileGroupTask) -> MultiBatchTask | list[MultiBatchTask]:
        rows: list[dict[str, Any]] = []
        storage_options = resolve_storage_options(io_kwargs=self.read_kwargs)

        for tar_path in task.data:
            with (
                fsspec.open(tar_path, mode="rb", **storage_options) as fobj,
                tarfile.open(fileobj=fobj, mode="r:*") as tf,
            ):
                members = [m for m in tf.getmembers() if m.isfile()]
                member_names = {m.name for m in members}
                read_ctx = _ReadContext(
                    tar_path=tar_path,
                    member_names=member_names,
                    member_info={m.name: m for m in members},
                    storage_options=storage_options,
                    byte_cache={},
                )
                for member in members:
                    if not member.name.endswith(self.json_extensions):
                        continue
                    rows.extend(self._rows_from_member(tf=tf, member=member, read_ctx=read_ctx))

        if rows:
            table = pa.Table.from_pylist(rows)
            table = table.cast(self._reconcile_schema(table.schema))
        else:
            table = pa.Table.from_pylist([], schema=self._empty_output_schema())
        splits = split_table_by_group_max_bytes(table, "sample_id", self.max_batch_bytes)
        batches: list[MultiBatchTask] = []
        for idx, split in enumerate(splits):
            task_id = f"{task.task_id}_processed" if len(splits) == 1 else f"{task.task_id}_processed_{idx:05d}"
            metadata = dict(task._metadata)
            if storage_options:
                metadata["source_storage_options"] = storage_options
            batches.append(
                MultiBatchTask(
                    task_id=task_id,
                    dataset_name=task.dataset_name,
                    data=split,
                    _metadata=metadata,
                    _stage_perf=task._stage_perf,
                )
            )
        return batches if len(batches) > 1 else batches[0]
